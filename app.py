import os
import base64
import logging
import gradio as gr
from client.zhipu_llm import ZhipuLLM
import datetime
from neo4j import GraphDatabase

neo_driver = GraphDatabase.driver(
    "bolt://localhost:7687",
    auth=("neo4j", "415zc415")
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s"
)
logger = logging.getLogger(__name__)
llm = ZhipuLLM()

DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
os.makedirs(DATA_DIR, exist_ok=True)

# CSSï¼šå·¦ä¾§ Ã— å›¾æ ‡ & æ‚¬åœé«˜äº®
css = """
#file-selector .gr-checkbox {
  padding: 8px 8px 8px 28px;
  position: relative;
  border-radius: 4px;
  transition: background-color 0.2s;
}
/* Ã— å›¾æ ‡æ”¾åœ¨å·¦ä¾§ */
#file-selector .gr-checkbox:hover::before {
  content: "Ã—";
  position: absolute;
  left: 8px;
  top: 50%;
  transform: translateY(-50%);
  color: #e00;
  font-size: 16px;
  cursor: pointer;
}
/* æ‚¬åœé«˜äº® */
#file-selector .gr-checkbox:hover {
  background-color: #f5f5f5;
}
/* é™åˆ¶æ¸…é™¤æŒ‰é’®å®½åº¦ */
#clear-btn {s
  min-width: 50px;
  max-width: 80px;
}
"""

QUESTIONS = [
    "1/4 æ‚¨æ˜¯å¦æœ‰å¤šå°¿ã€å¤šé¥®ã€å¤šé£Ÿç­‰é«˜è¡€ç³–ç—‡çŠ¶ï¼Ÿï¼ˆæœ‰ / æ— ï¼‰",
    "2/4 æ‚¨çš„ç©ºè…¹è¡€ç³–ï¼ˆFPGï¼‰æ˜¯å¤šå°‘ï¼Ÿï¼ˆmmol/Lï¼‰",
    "3/4 æ‚¨çš„é¤å 2 å°æ—¶è¡€ç³–æ˜¯å¤šå°‘ï¼Ÿï¼ˆmmol/Lï¼‰",
    "4/4 æ‚¨çš„ HbA1cï¼ˆç³–åŒ–è¡€çº¢è›‹ç™½ï¼‰æ˜¯å¤šå°‘ï¼Ÿï¼ˆ%ï¼‰",
]

def _query_graph(answers: dict):
    """
    answers = {"sym": "æœ‰/æ— ", "fpg": "7.8", "pp2h": "12.0", "hb": "6.8"}
    è¿”å›å»é‡åçš„è¯Šæ–­åç§°åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    """
    cypher = """
    WITH $sym AS sym,
         toFloat($fpg)  AS fpg,
         toFloat($pp2h) AS pp2h,
         toFloat($hb)   AS hb
    OPTIONAL MATCH (s:Symptom {name:'å…¸å‹é«˜è¡€ç³–ç—‡çŠ¶'})-[:LEADS_TO]->(d1:Diagnosis)
      WHERE sym = 'æœ‰'
    OPTIONAL MATCH (i1:Indicator {name:'FPG'})-[:LEADS_TO]->(d2:Diagnosis)
      WHERE fpg >= i1.threshold
    OPTIONAL MATCH (i2:Indicator {name:'PostPrandial2H'})-[:LEADS_TO]->(d3:Diagnosis)
      WHERE pp2h >= i2.threshold
    OPTIONAL MATCH (i3:Indicator {name:'HbA1c'})-[:LEADS_TO]->(d4:Diagnosis)
      WHERE hb >= i3.threshold
    RETURN
      collect(d1.name) + collect(d2.name) +
      collect(d3.name) + collect(d4.name) AS all_names
    """
    with neo_driver.session(bookmarks=None) as s:
        rec = s.run(cypher, **answers).single()
        raw = rec["all_names"] if rec else []
        # è¿‡æ»¤ None / ç©ºä¸²ï¼Œå†å»é‡
        return list({n for n in raw if n})

def gen_followup_question(answers: dict, asked: list[str]) -> str:
    """
    answers : å·²æ”¶é›†çš„å›ç­” dict
    asked   : å·²ç»é—®è¿‡çš„é—®é¢˜æ–‡æœ¬åˆ—è¡¨
    """
    prompt = (
        "ä½ æ˜¯ä¸€åå†…åˆ†æ³Œç§‘åŒ»ç”Ÿï¼Œæ­£åœ¨åšç³–å°¿ç—…é—®è¯Šã€‚\n"
        "ä»¥ä¸‹é—®é¢˜å·²ç»é—®è¿‡ï¼Œç¦æ­¢é‡å¤ï¼š\n" +
        "\n".join(f"- {q}" for q in asked) + "\n\n"
        "å·²è·å›ç­”ï¼š\n"
        f"å…¸å‹ç—‡çŠ¶={answers.get(0,'æœªå›ç­”')}ï¼›"
        f"FPG={answers.get(1,'æœªå›ç­”')}ï¼›"
        f"é¤å2h={answers.get(2,'æœªå›ç­”')}ï¼›"
        f"HbA1c={answers.get(3,'æœªå›ç­”')}\n"
        "è¯·æå‡ºä¸‹ä¸€æ¡æœ€å…³é”®ä¸”ä¸é‡å¤çš„é—®é¢˜ï¼Œ"
        "è¦æ±‚ï¼šç”¨ä¸­æ–‡ã€ç®€æ´ï¼Œä¸”åªè¾“å‡ºé—®é¢˜æœ¬èº«ã€‚"
    )
    try:
        q = llm._call(prompt).strip()
    except Exception:
        q = ""
    return q or "è¯·æä¾›å…¶ä»–ä¸è¡€ç³–ç›¸å…³çš„æ£€æŸ¥æˆ–ç—‡çŠ¶ä¿¡æ¯ï¼Ÿ"

def guided_on_send(text, file_list, history_state,
                   name, age, weight, gender, past_history):
    history, step, answers = history_state
    user_text = (text or "").strip()

    # æ”¶é›†ä¸Šä¸€æ­¥å›ç­”
    if step > 0:
        answers[step-1] = user_text
        history.append({"role":"user","content":user_text})

    # è¿˜æ²¡é—®å®Œ â‡’ ç»§ç»­æé—®
    if step < len(QUESTIONS):
        q = QUESTIONS[step]
        history.append({"role":"assistant","content":q})
        step += 1
        return history, (history, step, answers), file_list, gr.update(), gr.update(value="")

    diag_list = _query_graph({
        "sym":  answers.get(0, "æ— "),
        "fpg":  answers.get(1, "0"),
        "pp2h": answers.get(2, "0"),
        "hb":   answers.get(3, "0")
    })
    MAX_EXTRA_QUESTIONS = 5   # æœ€å¤šç»§ç»­è¿½é—® 5 æ¬¡
    MAX_RETRY = 3          # ç”Ÿæˆä¸é‡å¤é—®é¢˜æœ€å¤šé‡è¯• 3 æ¬¡
    
    if diag_list:
        reply = "åˆæ­¥å¯èƒ½è¯Šæ–­ç±»å‹ï¼š" + "ã€".join(diag_list)
        history.append({"role":"assistant","content":reply})
        # ç»“æŸï¼šé‡ç½® state
        new_state = ([{"role":"assistant","content":"å¦‚éœ€å†æ¬¡é—®è¯Šï¼Œè¯·è¾“å…¥ä»»æ„å†…å®¹ã€‚"}], 0, {})
        return history, new_state, [], gr.update(), gr.update(value="")

    # â”€â”€ è‹¥ä»æœªè¯Šæ–­ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸‹ä¸€æ¡é—®é¢˜ â”€â”€
    if step - 4 >= MAX_EXTRA_QUESTIONS:
        reply = "å·²è¿½é—®å¤šæ¬¡ä»æ— æ³•ç»™å‡ºè¯Šæ–­ï¼Œå»ºè®®æºå¸¦å®Œæ•´æ£€æŸ¥æŠ¥å‘Šå°±åŒ»ã€‚"
        history.append({"role":"assistant","content":reply})
        new_state = ([{"role":"assistant","content":"å¦‚éœ€å†æ¬¡é—®è¯Šï¼Œè¯·è¾“å…¥ä»»æ„å†…å®¹ã€‚"}], 0, {})
        return history, new_state, [], gr.update(), gr.update(value="")

    asked_texts = [m["content"] for m in history if m["role"] == "assistant" and m["content"].endswith("ï¼Ÿ")]
    for _ in range(MAX_RETRY):
        next_q = gen_followup_question(answers, asked_texts)
        if next_q not in asked_texts:
            break
    else:  # ä¸‰æ¬¡éƒ½é‡å¤ï¼Œå°±ç»™å…œåº•é—®é¢˜
        next_q = "è¯·å‘Šè¯‰æˆ‘ä»»ä½•æœ€è¿‘è¡€ç³–ç›¸å…³çš„å¼‚å¸¸æ£€æŸ¥é¡¹ç›®ï¼Ÿ"

    history.append({"role":"assistant","content":next_q})
    step += 1
    return history, (history, step, answers), file_list, gr.update(), gr.update(value="")



def on_file_upload(file_paths, history, file_list):
    history   = history   or []
    file_list = file_list or []

    # å¦‚æœæ²¡é€‰æ–°æ–‡ä»¶ï¼Œä»…åˆ·æ–°åˆ—è¡¨
    if not file_paths:
        opts = [os.path.basename(p) for p in file_list]
        return history, history, file_list, gr.update(choices=opts, value=[])

    paths = file_paths if isinstance(file_paths, list) else [file_paths]

    from tools.lab_report_parser import parse_lab_report

    for p in paths:
        if p in file_list:
            continue
        file_list.append(p)
        name = os.path.basename(p)
        ext  = os.path.splitext(name)[1].lower().lstrip(".")

        with open(p, "rb") as f:
            file_bytes = f.read()
            b64 = base64.b64encode(file_bytes).decode("utf-8")

        # èŠå¤©åŒºæ’å…¥å›¾ç‰‡/æ–‡ä»¶
        if ext in ("png","jpg","jpeg"):
            md = f"![{name}](data:image/{ext};base64,{b64})"
            history.append({"role":"system", "content":f"å·²ä¸Šä¼ å›¾ç‰‡ï¼š{name}\n\n{md}"})
            # è‡ªåŠ¨è§£æå›¾ç‰‡åŒ»å­¦æŒ‡æ ‡
            result = parse_lab_report(file_bytes)
            # è‡ªåŠ¨éå†æ‰€æœ‰éç©ºåŒ»å­¦æŒ‡æ ‡å¹¶å±•ç¤ºï¼ˆç›´æ¥ç”¨ä¸­æ–‡keyï¼‰
            if any(result.values()):
                summary = []
                for k, v in result.items():
                    if v is not None:
                        summary.append(f"{k}: {v}")
                if summary:
                    history.append({"role": "system", "content": "è‡ªåŠ¨è¯†åˆ«ä¿¡æ¯ï¼š\n" + "\n".join(summary)})
        elif ext == "pdf":
            md = f"[ğŸ“„ {name}](data:application/pdf;base64,{b64})"
            history.append({"role":"system","content":f"å·²ä¸Šä¼  PDFï¼š{md}"})
            # è‡ªåŠ¨è§£æ PDF åŒ»å­¦æŒ‡æ ‡
            result = parse_lab_report(file_bytes)
            if any(result.values()):
                summary = []
                for k, v in result.items():
                    if v is not None:
                        summary.append(f"{k}: {v}")
                if summary:
                    history.append({"role": "system", "content": "è‡ªåŠ¨è¯†åˆ«ä¿¡æ¯ï¼š\n" + "\n".join(summary)})
        else:
            history.append({"role":"system","content":f"å·²ä¸Šä¼ æ–‡ä»¶ï¼š{name}"})

    opts = [os.path.basename(p) for p in file_list]
    return history, history, file_list, gr.update(choices=opts, value=[])

def on_delete(selected, file_list):
    # ç‚¹å‡»å‹¾é€‰å³åˆ é™¤
    file_list = file_list or []
    remaining = [p for p in file_list if os.path.basename(p) not in (selected or [])]
    opts = [os.path.basename(p) for p in remaining]
    return remaining, gr.update(choices=opts, value=[])

def on_send(text, file_list, history, name, age, weight, gender, past_history):
    history   = history or []
    user_msg  = text or ""
    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡/æŠ¥å‘Šè‡ªåŠ¨è¯†åˆ«ä¿¡æ¯
    auto_info = None
    for m in reversed(history):
        if m["role"] == "system" and m["content"].startswith("è‡ªåŠ¨è¯†åˆ«ä¿¡æ¯ï¼š"):
            auto_info = m["content"]
            break
    if file_list:
        names = ", ".join(os.path.basename(p) for p in file_list)
        user_msg = (user_msg + "\n" if user_msg else "") + f"[å·²ä¸Šä¼ æ–‡ä»¶ï¼š{names}]"
    # ç”¨æˆ·æ¶ˆæ¯ç›´æ¥ä¼ é€’
    history.append({"role":"user","content":user_msg})

    # LLM å»ºè®®
    if auto_info:
        # æœ‰è‡ªåŠ¨è¯†åˆ«ä¿¡æ¯ï¼Œä¼˜å…ˆè®©LLMåŸºäºå›¾ç‰‡/æŠ¥å‘Šç»“æ„åŒ–å†…å®¹ç»™å»ºè®®
        prompt = (
            f"ç”¨æˆ·ä¸Šä¼ äº†åŒ»å­¦æŠ¥å‘Šæˆ–å›¾ç‰‡ï¼Œç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«å‡ºå¦‚ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š\n{auto_info}\n"
            f"è¯·åŸºäºè¿™äº›åŒ»å­¦ä¿¡æ¯ï¼Œç»“åˆç”¨æˆ·æ¶ˆæ¯â€œ{user_msg}â€ï¼Œç»™å‡ºä¸“ä¸šçš„ç³–å°¿ç—…æ£€æµ‹/ç®¡ç†å»ºè®®ã€‚"
            "å¦‚æœä¿¡æ¯ä¸å…¨å¯é€‚å½“è¯´æ˜ï¼Œä½†ä¸è¦è¯´æ— æ³•è¯†åˆ«å›¾ç‰‡ã€‚"
        )
    else:
        prompt = f"ç”¨æˆ·æ¶ˆæ¯ï¼š{user_msg}\nè¯·åŸºäºæ­¤ç»™å‡ºä¸“ä¸šçš„ç³–å°¿ç—…æ£€æµ‹/ç®¡ç†å»ºè®®ã€‚"
    logger.info("Prompt to LLM: %s", prompt)
    try: reply = llm._call(prompt)
    except Exception as e: reply = f"æ¨¡å‹è°ƒç”¨å‡ºé”™ï¼š{e}"
    history.append({"role":"assistant","content":reply})

    # å‘é€åæ¸…ç©ºå·²ä¸Šä¼ åˆ—è¡¨ï¼Œä¸å†è‡ªåŠ¨ç”Ÿæˆç—…ä¾‹
    return history, history, [], gr.update(choices=[], value=[]), gr.update(value="")

def on_generate_case(history, name=None, age=None, weight=None, gender=None, past_history=None):
    if not history or len(history) == 0:
        return "**ç—…ä¾‹è®°å½•**\n\nå°šæ— å†…å®¹"
    hist = "\n".join(f"{m['role']}: {m['content']}" for m in history)
    personal_info = (
        f"å§“åï¼š{name or 'æœªå¡«å†™'}ï¼›å¹´é¾„ï¼š{age or 'æœªå¡«å†™'}ï¼›ä½“é‡ï¼š{weight or 'æœªå¡«å†™'}ï¼›"
        f"æ€§åˆ«ï¼š{gender or 'æœªå¡«å†™'}ï¼›æ—¢å¾€å²ï¼š{past_history or 'æœªå¡«å†™'}"
    )
    case_p = (
        f"è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å’Œä¸ªäººä¿¡æ¯ç”Ÿæˆç»“æ„åŒ–ç³–å°¿ç—…ç—…ä¾‹ï¼š\n\n"
        f"{personal_info}\n\n{hist}\n\n"
        "ç—…ä¾‹åº”åŒ…æ‹¬ï¼šç”¨æˆ·ä¸ªäººä¿¡æ¯(å§“åï¼Œå¹´é¾„ï¼Œä½“é‡ï¼Œæ€§åˆ«)ã€ä¸»è¯‰ã€ç°ç—…å²ã€æ—¢å¾€å²ã€æ£€æŸ¥ç»“æœã€åˆæ­¥è¯Šæ–­ã€ç®¡ç†å»ºè®®ã€‚æ§åˆ¶å­—æ•°åœ¨500å­—ä¹‹å†…"
    )
    logger.info("Case prompt to LLM: %s", case_p)
    try:
        case = llm._call(case_p)
    except Exception as e:
        case = f"ç”Ÿæˆç—…ä¾‹å‡ºé”™ï¼š{e}"
        return case
    return case

def on_clear_history():
    welcome_msg = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç³–å°¿ç—…ä¸“ä¸šåŠ©æ‰‹ï¼Œè¯·æ‚¨æä¾›è¯¦ç»†ç—…ä¾‹ä¿¡æ¯ï¼Œä»¥ä¾¿æˆ‘ä¸ºæ‚¨é‡èº«å®šåˆ¶åŒ»å­¦å»ºè®®ã€‚"}]
    return welcome_msg, welcome_msg, "**ç—…ä¾‹è®°å½•**\n\nå°šæ— å†…å®¹"

with gr.Blocks(css=css) as demo:
    gr.Markdown("## ç³–å°¿ç—…åŠ©æ‰‹ ğŸ©¸ â€” å·¦ï¼šå¯¹è¯äº¤äº’ï¼›å³ï¼šç—…ä¾‹è®°å½•")

    # æ–°å¢ï¼šä¸ªäººä¿¡æ¯è¾“å…¥æ¡†
    with gr.Row():
        name_input = gr.Textbox(label="å§“å", placeholder="è¯·è¾“å…¥å§“å", lines=1)
        age_input = gr.Textbox(label="å¹´é¾„", placeholder="è¯·è¾“å…¥å¹´é¾„", lines=1)
        weight_input = gr.Textbox(label="ä½“é‡ï¼ˆkgï¼‰", placeholder="è¯·è¾“å…¥ä½“é‡", lines=1)
        gender_input = gr.Dropdown(label="æ€§åˆ«", choices=["ç”·", "å¥³"], value=None)
        history_input = gr.Textbox(label="æ—¢å¾€å²", placeholder="è¯·è¾“å…¥æ—¢å¾€å²", lines=1)

    # åˆå§‹å¼•å¯¼æ¶ˆæ¯
    initial_message = {
        "role": "assistant",
        "content": (
            "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½ç³–å°¿ç—…å¥åº·ç®¡ç†åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ç³–å°¿ç—…ç›¸å…³çš„æ£€æµ‹è§£è¯»ã€å¥åº·å»ºè®®å’Œä¸ªæ€§åŒ–ç®¡ç†æ–¹æ¡ˆã€‚\n"
            "è¯·é—®æ‚¨çš„å§“åã€å¹´é¾„ã€æ€§åˆ«ã€ç³–å°¿ç—…ç±»å‹ã€è¯Šæ–­æ—¶é—´ç­‰åŸºæœ¬ä¿¡æ¯ï¼Œä»¥åŠç›®å‰çš„ä¸»è¦å¥åº·å…³æ³¨ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
        )
    }

    with gr.Row():
        # å·¦ä¾§å¯¹è¯åŒºåŸŸ
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(type="messages", label="å¯¹è¯è®°å½•", height=500)
            with gr.Row():
                upload_btn = gr.UploadButton(
                    "ğŸ“ ä¸Šä¼ æ–‡ä»¶",
                    file_types=[".png",".jpg",".jpeg",".pdf"],
                    file_count="multiple",
                    type="filepath",
                    elem_id="upload-btn",
                    scale=1
                )
                text_input = gr.Textbox(
                    placeholder="è¯·è¾“å…¥é—®é¢˜æˆ–å¤‡æ³¨ï¼ˆå¯é€‰ï¼‰",
                    lines=1,
                    show_label=False,
                    elem_id="text-input",
                    scale=2
                )
                send_btn = gr.Button("å‘é€", elem_id="send-btn", scale=1)
            file_list = gr.State([])  # ç”¨äºå­˜å‚¨æ–‡ä»¶è·¯å¾„
            file_selector = gr.CheckboxGroup(
                choices=[],
                label="å·²ä¸Šä¼ æ–‡ä»¶ï¼ˆç‚¹å‡» Ã— åˆ é™¤ï¼‰",
                elem_id="file-selector"
            )
            with gr.Row():
                gr.Examples(
                    examples=[
                        "ç³–å°¿ç—…å¦‚ä½•æ§åˆ¶è¡€ç³–ï¼Ÿ",
                        "èƒ°å²›ç´ ä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼Ÿ",
                        "ä½è¡€ç³–å¤„ç†æ–¹å¼",
                        "æˆ‘æœ€è¿‘è¡€ç³–æœ‰ç‚¹é«˜ï¼Œæ€ä¹ˆç¼“è§£ï¼Ÿ",
                        "ç³–å°¿ç—…é¥®é£Ÿæœ‰å“ªäº›ç¦å¿Œï¼Ÿ",
                        "è¿åŠ¨å¯¹è¡€ç³–å½±å“",
                        "å¦‚ä½•ç›‘æµ‹è¡€ç³–å˜åŒ–ï¼Ÿ",
                        "ç³–å°¿ç—…å¹¶å‘ç—‡æœ‰å“ªäº›ï¼Ÿ",
                        "èƒ°å²›ç´ æ³µçš„é€‚ç”¨æ€§",
                        "è¡€ç³–é«˜æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ",
                    ],
                    inputs=[text_input]
                )
                clear_btn = gr.Button("æ¸…é™¤å¯¹è¯å†å²", elem_id="clear-btn", scale=1)

        # å³ä¾§ç—…ä¾‹è®°å½•
        with gr.Column(scale=2):
            case_md = gr.Markdown("**ç—…ä¾‹è®°å½•**\n\nå°šæ— å†…å®¹")
            gen_case_btn = gr.Button("ç”Ÿæˆç—…ä¾‹æŠ¥å‘Šå•", elem_id="gen-case-btn")

    init_msg = [{"role":"assistant","content":"æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç³–å°¿ç—…æ™ºèƒ½é—®è¯ŠåŠ©æ‰‹ï¼Œå¼€å§‹é—®è¯Šã€‚"}]
    state = gr.State((init_msg, 0, {}))   # (history, step, answers)

    # ä¸Šä¼  -> æ›´æ–°èŠå¤© & æ–‡ä»¶åˆ—è¡¨
    upload_btn.upload(
        fn=on_file_upload,
        inputs=[upload_btn, state, file_list],
        outputs=[chatbot, state, file_list, file_selector]
    )
    # å‹¾é€‰å³åˆ é™¤
    file_selector.change(
        fn=on_delete,
        inputs=[file_selector, file_list],
        outputs=[file_list, file_selector]
    )
    # å‘é€ -> ç”Ÿæˆå›å¤ï¼Œå¹¶æ¸…ç©ºæ–‡ä»¶åˆ—è¡¨å’Œè¾“å…¥æ¡†
    send_btn.click(
        fn=guided_on_send,
        inputs=[text_input, file_list, state, name_input, age_input, weight_input, gender_input, history_input],
        outputs=[chatbot, state, file_list, file_selector, text_input]
    )
    text_input.submit(
        fn=guided_on_send,
        inputs=[text_input, file_list, state, name_input, age_input, weight_input, gender_input, history_input],
        outputs=[chatbot, state, file_list, file_selector, text_input]
    )
    # æ¸…é™¤å¯¹è¯å†å²æŒ‰é’®
    clear_btn.click(
        fn=on_clear_history,
        inputs=None,
        outputs=[chatbot, state, case_md]
    )
    # ç”Ÿæˆç—…ä¾‹æŠ¥å‘Šå•æŒ‰é’®
    gen_case_btn.click(
        fn=on_generate_case,
        inputs=[state, name_input, age_input, weight_input, gender_input, history_input],
        outputs=[case_md]
    )

if __name__ == "__main__":
    demo.launch(inbrowser=True)
